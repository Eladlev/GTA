from opencompass.models import HuggingFaceCausalLM, OpenAI
from opencompass.models.internal import LLama, InternLMwithModule

api_meta_template = dict(
    round=[
            dict(role='HUMAN', api_role='HUMAN'),
            dict(role='BOT', api_role='BOT', generate=True),
    ],
)

models = [
    # dict(
    #     abbr="baichuan2_7b_base_00220",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_00220B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_00440",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_00440B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_00660",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_00660B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_00880",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_00880B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_01100",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_01100B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_01320",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_01320B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_01540",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_01540B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_01760",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_01760B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_01980",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_01980B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_02200",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_02200B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),
    # dict(
    #     abbr="baichuan2_7b_base_02420",
    #     type=InternLMwithModule,
    #     model_type="BAICHUAN2",
    #     path="/mnt/petrelfs/share_data/common_share/baichuan2_7b_base_intermediate_checkpoints/train_02420B/",
    #     tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/baichuan2.model',
    #     tokenizer_type='llama',
    #     module_path="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/",
    #     model_config="/mnt/petrelfs/share_data/yangxiaogui/train_internlm/configs/_base_/models/baichuan2/baichuan2_7B.py",
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     run_cfg=dict(num_gpus=1, num_procs=1)
    # ),

    # dict(
    #     abbr="openllama-3b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/chenkeyu1/models/huggingface/open_llama_3b",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               trust_remote_code=True,
    #                               # use_fast=False,
    #                               ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto',
    #                       trust_remote_code=True,
    #                       ),
    #     batch_padding=True,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="openllama-7b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/chenkeyu1/models/huggingface/open_llama_7b",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           # use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto',
    #                       trust_remote_code=True,
    #                       ),
    #     batch_padding=True,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="openllama-13b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/chenkeyu1/models/huggingface/open_llama_13b",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           # use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto',
    #                       trust_remote_code=True,
    #                       ),
    #     batch_padding=True,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=2, num_procs=1),
    # ),

    # dict(
    #     abbr="llama-7b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/llama2_hf/llama-2-7b-hf",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="llama-13b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/llama2_hf/llama-2-13b-hf",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=2, num_procs=1),
    # ),
    #
    # dict(
    #     abbr="llama-70b",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/llama2_hf/llama-2-70b-hf",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=8, num_procs=1),
    # ),
    # dict(abbr='GPT4',
    #      type=OpenAI, path='gpt-4-0613',
    #      key='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',  # The key will be obtained from $OPENAI_API_KEY, but you can write down your key here as well
    #      meta_template=api_meta_template,
    #      query_per_second=1,
    #      max_out_len=2048, max_seq_len=2048, batch_size=8),
]


models+=[
    dict(abbr="LLama2-7B",
         type=LLama, path='/mnt/petrelfs/share_data/llm_llama/llama2_raw/llama-2-7b',
         tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/llama.model', tokenizer_type='llama',
         max_out_len=100, max_seq_len=2048, batch_size=16, run_cfg=dict(num_gpus=1, num_procs=1)),
    # dict(abbr="LLama2-13B",
    #      type=LLama, path='/mnt/petrelfs/share_data/llm_llama/llama2_raw/llama-2-13b',
    #      tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/llama.model', tokenizer_type='llama',
    #      max_out_len=100, max_seq_len=2048, batch_size=16, run_cfg=dict(num_gpus=2, num_procs=2)),
    #
    # dict(abbr='GPT3.5',
    #      type=OpenAI, path='gpt-3.5-turbo',
    #      key='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',  # The key will be obtained from $OPENAI_API_KEY, but you can write down your key here as well
    #      meta_template=api_meta_template,
    #      query_per_second=1,
    #      max_out_len=2048, max_seq_len=2048, batch_size=8),
    # dict(
    #     abbr="qwen2-7B",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/Qwen/Qwen-7B_9_25",
    #     # tokenizer_path='chiayewken/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="qwen2-14B",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/Qwen/Qwen-14B_9_25",
    #     # tokenizer_path='chiayewken/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=2, num_procs=1),
    # ),
    # dict(
    #     abbr="chatglm",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/chatglm3/",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),

]