from opencompass.models import HuggingFaceCausalLM, OpenAI
from opencompass.models.internal import LLama, InternLM

api_meta_template = dict(
    round=[
        dict(role='HUMAN', api_role='HUMAN'),
        dict(role='BOT', api_role='BOT', generate=True),
    ],
)

models = [
    dict(abbr="LLama2-7B",
         type=LLama, path='/mnt/petrelfs/share_data/llm_llama/llama2_raw/llama-2-7b',
         tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/llama.model', tokenizer_type='llama',
         max_out_len=100, max_seq_len=2048, batch_size=16, run_cfg=dict(num_gpus=1, num_procs=1)),
    # dict(abbr="LLama2-13B",
    #      type=LLama, path='/mnt/petrelfs/share_data/llm_llama/llama2_raw/llama-2-13b',
    #      tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/llama.model', tokenizer_type='llama',
    #      max_out_len=100, max_seq_len=2048, batch_size=16, run_cfg=dict(num_gpus=2, num_procs=2)),
    # dict(abbr='GPT3.5',
    #      type=OpenAI, path='gpt-3.5-turbo',
    #      key='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',
    #      # The key will be obtained from $OPENAI_API_KEY, but you can write down your key here as well
    #      meta_template=api_meta_template,
    #      query_per_second=1,
    #      max_out_len=2048, max_seq_len=2048, batch_size=8),
    # dict(abbr='GPT4.0',
    #      type=OpenAI, path='gpt-4-0613',
    #      key='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',
    #      # The key will be obtained from $OPENAI_API_KEY, but you can write down your key here as well
    #      meta_template=api_meta_template,
    #      query_per_second=1,
    #      max_out_len=2048, max_seq_len=2048, batch_size=8),
    # dict(
    #     abbr="qwen2-7B",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/Qwen/Qwen-7B_9_25",
    #     # tokenizer_path='chiayewken/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="qwen2-14B",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/Qwen/Qwen-14B_9_25",
    #     # tokenizer_path='chiayewken/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,
    #     pad_token_id=0,
    #     run_cfg=dict(num_gpus=2, num_procs=1),
    # ),
    #
    # dict(
    #     abbr="chatglm2",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/chatglm2-6b",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="chatglm3",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/share_data/feizhaoye/huggingface/chatglm3",
    #     # tokenizer_path='Qwen/Qwen-7B',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           trust_remote_code=True,
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto', trust_remote_code=True,
    #                       ),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
dict(
    abbr="baichuan_7b",
    type=HuggingFaceCausalLM,
    path="/mnt/petrelfs/share_data/sunyu2/baichuan-7b",
    tokenizer_kwargs=dict(trust_remote_code=True,
                          use_fast=False),
    max_out_len=100,
    max_seq_len=2048,
    batch_size=16,
    model_kwargs=dict(device_map='auto', trust_remote_code=True),
    batch_padding=False,
    pad_token_id=0,
    run_cfg=dict(num_gpus=1, num_procs=1)
),
dict(
    abbr="baichuan_13b",
    type=HuggingFaceCausalLM,
    path="/mnt/petrelfs/share_data/sunyu2/baichuan-13b",
    tokenizer_kwargs=dict(trust_remote_code=True, use_fast=False),
    max_out_len=100,
    max_seq_len=2048,
    batch_size=16,
    model_kwargs=dict(device_map='auto', trust_remote_code=True),
    batch_padding=False,
    pad_token_id=0,
    run_cfg=dict(num_gpus=2, num_procs=1)
),
# dict(
#         abbr="baichuan2-7B",
#         type=HuggingFaceCausalLM,
#         path="/mnt/petrelfs/share_data/chenkeyu1/models/baichuan/baichuan/baichuan2_7B",
#         # tokenizer_path='chiayewken/Qwen-7B',
#         tokenizer_kwargs=dict(padding_side='left',
#                               truncation_side='left',
#                               trust_remote_code=True,
#                               use_fast=False,
#                               # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
#                               ),
#         max_out_len=100,
#         max_seq_len=2048,
#         batch_size=16,
#         model_kwargs=dict(device_map='auto', trust_remote_code=True,
#                           ),
#         batch_padding=False,
#         pad_token_id=0,
#         run_cfg=dict(num_gpus=1, num_procs=1),
#     ),
#     dict(
#         abbr="baichuan2-13B",
#         type=HuggingFaceCausalLM,
#         path="/mnt/petrelfs/share_data/chenkeyu1/models/baichuan/baichuan/baichuan2_13B",
#         # tokenizer_path='chiayewken/Qwen-7B',
#         tokenizer_kwargs=dict(padding_side='left',
#                               truncation_side='left',
#                               trust_remote_code=True,
#                               use_fast=False,
#                               # revision='39fc5fdcb95c8c367bbdb3bfc0db71d96266de09'
#                               ),
#         max_out_len=100,
#         max_seq_len=2048,
#         batch_size=16,
#         model_kwargs=dict(device_map='auto', trust_remote_code=True,
#                           ),
#         batch_padding=False,
#         pad_token_id=0,
#         run_cfg=dict(num_gpus=2, num_procs=1),
#     ),
#
    dict(
        abbr="internlm-7B",
        type=HuggingFaceCausalLM,
        path="/mnt/petrelfs/share_data/xingshuhao/internlm-7b",
        # tokenizer_path='Qwen/Qwen-7B',
        tokenizer_kwargs=dict(padding_side='left',
                              truncation_side='left',
                              trust_remote_code=True,
                              use_fast=False,
                              ),
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        model_kwargs=dict(device_map='auto', trust_remote_code=True,
                          ),
        batch_padding=False,  # if false, inference with for-loop without batch padding
        run_cfg=dict(num_gpus=1, num_procs=1),
    ),
#     dict(
#         abbr="InternLM20B",
#         type=InternLM,
#         path="/mnt/petrelfs/share_data/yanhang/ckpts/Newton_20B_0.4.3@3500/",
#         tokenizer_path='/mnt/petrelfs/share_data/yanhang/tokenizes/V7.model',
#         tokenizer_type='v7',
#         module_path="/mnt/petrelfs/share_data/yanhang/codes/Newton_20B_0.4.3@3500",
#         model_config="/mnt/petrelfs/share_data/chenkeyu1/Newton_20B_0-4-3.py",
#         max_out_len=100,
#         max_seq_len=2048,
#         batch_size=16,
#         run_cfg=dict(num_gpus=4, num_procs=4)
#     ),
]
